
<head>
  <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="style.css">

<title>Matthew Whitehead</title>
</head>


<body>

<center>
  <img src="media/neural_net_cut2.png" width=1000 style="top: -10px; opacity: 100%">

  <div class="main">


<br><br><br>
<figure>
  <img src="media/me.jpg" width=150 style="top: -10px;">
</figure>
<p class="titles">
  <b>Matthew Whitehead, Ph.D.<br>
    <!-- <a href="matthew_whitehead_cv.pdf">Academic CV</a> -->
    <a href="matthew_whitehead_resume.pdf">Resume</a>
    <a href="mailto:matthewemerynealwhitehead@gmail.com">Contact</a>

     <br>
   </b>
</p>

<br><br>
<hr>
<br><br>


<!--***************************************************-->




<p class="main">
    <b>About me</b>

        <figure>
          <img src="projects/deepq/tetris.gif" width=50>
          <figcaption>My DQN playing Tetris</figcaption>
        </figure>

<p class="main">
    My name is Matthew Whitehead and I am a tenured Associate Professor and Co-Chair in the Department of Mathematics and Computer Science at Colorado College.
</p>

<p class="main">
I work in applied machine learning including applications in natural language processing, sentiment mining, recommendation systems, deep learning, clustering, reinforcement learning, and computer vision.
</p>


<p class="main">
Below I briefly describe several selected projects I have worked on recently and link to PDFs of related academic publications.
</p>


<!-- Most recently I have been interested in learning more about attention techniques (transformers) and how they might be used to solve sequential algorithmic problems. -->

<br>
<hr>
<br>

<p class="main">

<b>Observational Neural Networks</b><br>

<figure>
  <img src="projects/observational_ann/mosaic.gif" width=300>
  <figcaption>Observing sorted hidden weight strengths for four ANNs<br>while they learn simple polynomial functions</figcaption>
</figure>

<p class="main">
This project involves training an ANN to observe the learning process (activations and change in weights) of another ANN.  The observer tries to figure out what the observee is trying to learn and then the hope is that it would able to either explain in higher-level terms what the observee is doing or offer suggestions for pre-existing library functions to call to solve the problem directly.
</p>

<p class="main">
Here's a simple example.  Let's say the observee is trying to learn the square root function.  The observer has already been trained watching other networks try to learn the sqrt function in the past, so it is able to match the current observee's training process and suggest a sqrt function from an existing library to solve the problem.
</p>

<p class="main">
One interesting aspect of this setup is that it doesn't even matter if ANNs are good at learning a particular function that exists in the library.  The observer could have seen how other ANNs tried to learn a particular function and then still be able to offer the actual library function to solve the problem.
</p>


<p class="main">
This is ongoing work and currently we're trying to determine how hard it is to differentiate an ANN's learning process for sqrt vs. tangent vs. sorting numbers, etc.  One technique we're currently investigating is using the graph diffusion distance metric to offer an alternative way to match learning processes to library functions.
</p>

<br>
<hr>
<br>


<!-- <p class="main">

<b>Function-Calling Neural Networks</b><br>
<i>Collaboration with Trevor Barron</i>

<figure>
  <img src="images/functioncalling2.png" width=450>
  <figcaption>A Function-Calling ANN</figcaption>
</figure>

<p class="main">
This project involves a transformer architecture that allows a model to leverage existing libraries of software functions to solve algorithmic problems more efficiently and in a more explainable way.
</p>

<p class="main">
The model uses transformer attention methods to choose low-level functions from a library of pre-written functions and then calls them in order to solve algorithmic problems.  For example, if the network is trying to learn to compute the GCD of two numbers, then it might employ the use of a basic subtraction function along with simple comparison functions to create an implementation of Euclid's Algorithm.  Learning which functions to call is then similar to deciding which part of the input data to pay attention to.
</p>



<br>
<hr>
<br> -->

<!--***************************************************-->




<p class="main">

<b>Natural Language Processing with US Patents</b><br>
<i>Collaboration with Daniel Johnson</i>


<figure>
  <img src="projects/nlp_patents/B29_B31_G06.png" width=300>
  <figcaption>t-SNE plot of patent documents for <br>several IPC classifications </figcaption>
</figure>


<!-- <p class="main">
in International Patent Classification groups B29 (Working of Plastics) (light blue), B31 (Making Paper Articles) (dark blue) and G06 (Computing, Calculating, and Counting) (orange).
</p> -->

<p class="main">
I worked with Daniel Johnson to use NLP on patent documents in a couple of different ways.  We first downloaded a dataset of the full texts of about 1.5 million US patents.  We then used word2vec to generate semantic word embeddings for the dataset and then used those embeddings to estimate patent creativity and breadth as complements to traditional economic measures of patent generality, originality, and significance.

<figure>
  <img src="projects/nlp_patents/patents.png" width=400>
  <figcaption>Example visualization of patent breadth metric.<br>
    The blue plot shows a <i>broader</i> patent.<br>
    The red plot shows a <i>narrower</i> patent.
  </figcaption>

</figure>

<p class="main">
We tested our new economic metrics on several hundred of Apple's patents.  Using the word embeddings for each patent document, we defined the patent's <i>breadth</i> as the hypervolume of its bounding box calculated across all embedding dimensions: &#x3a0; (max_i - min_i).  We defined each patent's measure of <i>creativity</i> as its document vector distance from the centroid of patent vectors in the same International Patent Classification (IPC) class.

<p class="main">
<a href="media/mypapers/apple.pdf">We published this work at the R & D Management Conference "From Science to Society: Innovation and Value Creation"</a>.
</p>

<p class="main">
We also used the patent document word embeddings to create a prototype online interactive visualization tool for analyzing the semantic proximities of US patent documents that are related to cancer research and treatments.
</p>

<figure>
  <img src="projects/nlp_patents/moonshot_screenshot.png" width=400>
  <figcaption>Cancer-related patent semantic search </figcaption>
</figure>

<p class="main">Our software allows the user to perform keyword searches and
then presents visualizations of sets of relevant patent documents clustered by semantic similarity. Semantic similarity is calculated using a combination of word embeddings obtained using the skip-gram algorithm and the t-SNE dimensionality reduction algorithm.
</p>

<p class="main">
The user may then select individual points in the cluster to view more detailed patent information. This process allows the user to explore the connections between related patents and see more general trends in the semantic shape of the technological space. It is our hope that this tool may serve as one of the starting points for data analysis leading to future innovative approaches to cancer treatment.
</p>

<p class="main">
This work <a href="media/mypapers/cancer_patents.pdf">was submitted in the USPTO Cancer Moonshot Competition</a> and received an honorable mention for <a href="https://www.uspto.gov/about-us/news-updates/uspto-announces-cancer-moonshot-challenge-winners">"the unique and innovative methodology applied."</a>
</p>

<br>
<hr>
<br>

<!--***************************************************-->




<p class="main">

<b>Deep Reinforcement Learning in a 3-D Environment</b><br>
<i>Collaboration with Trevor Barron and Alan Yeung</i>

<figure>
  <img src="projects/deepq/network1.png" width=350>
  <figcaption>CNN for DQN<br>
(2 to 16 conv. layers used)
  </figcaption>
</figure>


<!-- <figure>
  <img src="projects/deepq/depth_grid2.png" width=250>
  <figcaption>Estimating distance to objects in 3 dimensions<br>
    Column 1: Input view<br>
    Column 2: Ground truth distances<br>
    Column 3: Estimated distances<br>
    Column 4: Estimation errors
  </figcaption>
</figure> -->

<figure>
  <video width="250" height="200" controls>
  <source src="projects/deepq/big_world_agent.mp4" type="video/mp4">
  </video>
   <figcaption>Strip-Miner Agent <br>(Reward for breaking "dirt" blocks)</figcaption>
</figure>


<!-- <figure>
  <img src="projects/deepq/big_world_agent.gif" width=250>
  <figcaption>Strip-Miner Agent <br>(Reward for breaking "dirt" blocks)</figcaption>
</figure> -->

<p class="main">
We built a Deep Q Network (DQN) reinforcement learning agent based on the highly successful <a href="https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf">DeepMind Atari research</a> that learned to complete a variety of control tasks in a 3-D blockworld environment similar to the popular game, Minecraft.
</p>

<p class="main">
The model used a deep convolutional neural network based on the well-known <a href="https://arxiv.org/abs/1409.1556">VGG architecture</a>.  We were interested to see if this type of reinforcement learning agent would be able to learn to navigate a more complex 3-dimensional space using only visual input.
</p>

<!-- <figure>
  <img src="projects/deepq/narrow_walkway.gif" width=250>
  <figcaption>Narrow-Walkway Agent <br>(Reward for forward progress)</figcaption>
</figure> -->

<figure>
  <video width="250" height="200" controls>
  <source src="projects/deepq/animation_deep_walkwaye.mp4" type="video/mp4">
  </video>
   <figcaption>Narrow-Walkway Agent <br>(Reward for forward progress)</figcaption>
</figure>

<p class="main">
The agent learned to follow a narrow winding path, collect a variety of blocks in a complex world, and climb to the top of nearby high mountains and it does this solely from processing the raw pixels of the game's visual display.  The agent was given a simple reward/penalty based on the task being learned.
</p>

<!-- <figure>
  <img src="projects/deepq/block_targeter.gif" width=250>
  <figcaption>Block-Targeter Agent</figcaption>
</figure> -->

<!-- <p class="main">
<a href="https://www.youtube.com/watch?v=6jlaBD9LCnM">This video</a> shows the results with a trained reinforcement learning agent.
</p> -->

<p class="main">
<b><a href="media/mypapers/blockworld.pdf">This paper at IJCAI's DeepRL workshop</a> describes our work in more detail.  This project was partially supported by a generous hardware grant from NVIDIA.</b>
</p>


<br>
<hr>
<br>

<!--***************************************************-->







<p class="main">

<b>Visualizing Stacked Autoencoder Language Learning </b><br>
<i>Collaboration with Trevor Barron</i>

<figure>
  <img src="projects/wordclouds/wordcloud.png" width=450>
  <figcaption>A wordcloud visualization of language learned by a deep ANN subgraph</figcaption>
</figure>

<p class="main">
We created a tool to investigate and visualize how some deep artificial neural networks learn language.  In particular, we used stacked autoencoder networks (networks that learn to reproduce their input as output under network size constraints) to learn word meanings by processing a large corpus of text data (Wikipedia).
</p>

<p class="main">

  After processing large amounts of text data, the networks learned different language
  features and we could then extract word embedding representations from their hidden layers. We took these
  trained networks and then generated images that helped show each of the learned
  language features. Our method was analogous to the methods often used in computer
  vision research, but since we were working with text data, we used a different visualization technique that incorporated word cloud images and t-SNE plots.  In particular, we used word clouds to see which words maximally activate certain artificial neurons within the model.  Then by examining the set of words associated with a single neuron we hoped to gain some insight as to the semantic features being identified by that neuron.
</p>

<p class="main">
<b><a href="media/mypapers/wordclouds.pdf">This paper at the European Symposium on Artificial Neural Networks</a> describes our work in more detail.
This project was partially supported by a generous hardware donation from NVIDIA.
</b>
</p>

<br>
<hr>
<br>

<!--***************************************************-->




</div>
